
ffmpeg 的学习过程笔记，会往其中放思维导图。

1.解码（将mp4文件视频数据解码成yuv等原始数据，对应的思维导图是 mydecode.png）：
AVFormatContext ：封装格式的容器 e.gMP4
AVCodecContext ：编解码中的相关数据，编解码格式的抽象
AVCodec ：存储编解码器的结构体，编解码器
AVStream 对流的抽象
AVFrame ：用来存储原始数据 视频的就是yuv，音频的就是pcm
AVOutputFormat:对输出文件格式的抽象

如果视频原始数据AV_PIX_FMT_YUV420P 就是平面格式的（yyyy uu vv ） ,(yyyy uu vv )。
但是我的锤子手机使用yv12最后拿到的yuv原始数据是nv12才能正确解析出来。
当如果发现你解析出来的界面蓝绿色对调了问题就是你的uv数据放反了。
如果是花屏了就是你的原始数据和解码器对应错了。比如你yv12的原始yuv数据，然后用nv12来解析，就是花屏

其他：malloc_usable_size（*）查看实际分配的内存大小


2.编码：（将yuv数据编码成h264文件，对应的思维导图是 my_encode_yuv_2_h264.png）
这里记录下导图中没有体现的细节
通过avformat_alloc_context创建的AVFormatContext
通过av_guess_format创建AVOutputFormat
使用   AVFormatContext->oformat = AVOutputFormat;来关联
avpicture_fill，为AvFrame来关联一个缓冲区，然后Avframe中的data[4]其实就是指向这个缓冲区的不同地方。要看你是原始数据的格式方式。


3.关于ffmepg中pts，dts，time_base中的问题：
先以视频帧来理解
pts就是视频显示的时间
dts就是被解码的时间
time_base ，就是一秒钟的一个，比如视频是一秒钟25帧，那么time_base就是=（1,25）=1/25;
如果是吧时间分成90000份，那么就是1/90000秒，此时的time_base={1,90000};
所以pts就是占多少个时间格子
在ffmpeg中有av_q2d(time_base),就是把分数转换成小数的
pts * av_q2d(time_base)才是帧的显示时间戳。
AVStream->duration就是这个视频流有多长
AVStream->duration * av_q2d(time_base) = time就是这个视频的长度（秒）
ffmpeg内部的时间和标准的时间转换就是一个AV_TIME_BASE
比如ffmepg内部的时间 = time * AV_TIME_BASE;

av_rescale_q(int64_t a, AVRational bq, AVRational cq) 这个是时间基的转换函数a * bq / cq
比如计算的时间他会有avcodec的时间基，读取的instream有时间基，写入文件的时候有outstream的时间基等等。都需要转换


4.音频录制（通过设备拿到pcm原始数据转换成aac存在本地 , 导图是audio_record.png）：
http://blog.csdn.net/XIAIBIANCHENG/article/details/72810495 格式转换
音频录制，首先要通过android上层的api（audiorecord）拿到数据，不过在jni成也能拿到数据.
在上层会初始化一些参数，我这边是单声道，44100的采样率，16bit的数据这是大多数的android机都支持的。
但是在ffmepg自带的编码器（当然可以在编译ffmpeg中支持对应的编码器，aac等）中是只支持AV_SAMPLE_FMT_FLTP。
所以在底层进行转换一下。但是如果是双声道的话，两者区别不大(但是不知道用AV_SAMPLE_FMT_FLTP来编码AV_SAMPLE_FMT_S16是否会成功)。
下面看看双声道
AV_SAMPLE_FMT_S16（data[0] lrlrlrlrlrlr）
AV_SAMPLE_FMT_FLTP(data[0] data[1] lllll rrrrr);
这个就要用SwrContext来转换下。具体内容看代码my_audio_record.c

frame->nb_samples，一帧音频（frame）中有多少个采样（一般是1024）
而一般1s中的音频是44100个采样。
一般一个采样的大小 AV_SAMPLE_FMT_S16（16比特，2字节）
所以单声道1s 的音频大小是 44100 * 2 个字节（byte）;
一帧的大小（frame）frame->nb_samples * av_get_bytes_per_sample(s16) 个字节（一般是2048）


音频的时间方面：

一帧 1024个 sample。采样率 Samplerate 44100KHz，每秒44100个AV_SAMPLE_FMT_S16, 所以根据公式
音频帧的播放时间=一个AAC帧对应的采样样本的个数/采样频率
当前AAC一帧的播放时间是= 1024*1000000/44100= 22.2ms(单位为ms)

调试发现，对音频重采样后，frame_->format中的格式并不会改变，
所以使用av_get_bytes_per_sample获取每个采样的大小的时候需要手动改成outFormat

opensl中的的缓冲播放完毕后会回调pcmcallback，
询问老师：第一次调用需要延迟个几百毫秒，然后需要保证队列中一直有数据

5.采集音视频并复合mp4（现在已经做好的还有点问题就是音频播放过快。但是还不知道如何改正）
这里面需要注意的一个点是av_compare_ts ，传入的是上一帧（音视频）的pts，time_base 是最后状态的pts，
比如音频是编码后的codec->time_base , 视频是编码后的video_stream->time_base.
这个音视频同步的问题感觉还没有个方向，先暂时放一会（xxxxxxxxxx）



6.音视频同步问题。
1.一般是视频去同步音频，因为音频有卡顿的话体验很差。
2.音视频两者的时间基不同，需要转换下才能比较两个的pts的时间
3.如果视频播放过快。需要sleep等待下音频



音视频缓冲中的困惑。
1.视频音频的缓冲帧数量大小都是100
 视频的100是四秒
 音频的100就两秒多 一帧有1024个采样，那么100帧音频占时 = 100 / (44100 / 1024) = 2.3 秒左右
 两个队列只要一个满了，都会造成av_read_frame（）线程的阻塞，会不会出现视频还是满的，但是音频已经空了的现象。然后音频的播放就会停止

老师回答：有这个可能，音视频交替存放的，一般不会出现一个满一个空，控制好缓冲大小应该能解决


从yuv420p中把yuv数据读取出来的方法
frame_->linesize[0]是根据不同的cpu来对其的，保证读写效率。可能比width大
for(int i = 0 ;i < vc->height ; ++i){
    memcpy(myData->datas[0] + vc->width * i,vframe->data[0] + vframe->linesize[0] * i ,  vc->width );
}
for(int i = 0 ;i < vc->height / 2 ; ++i){
    memcpy(myData->datas[1] + vc->width / 2 * i ,vframe->data[1] + vframe->linesize[1] * i ,  vc->width / 2 );
}
for(int i = 0 ;i < vc->height / 2 ; ++i){
    memcpy(myData->datas[2]  + vc->width / 2 * i,vframe->data[2] + vframe->linesize[2] * i ,  vc->width / 2);
}
linesize 可能会比width大，因为是为了读取效率，根据不同的cpu有不同的对齐方式。
按这个算法反推是应该是右边有内存舍弃（就是每一行的右边有舍弃部分）


视频seek，比如一个视频fps 25，那么两帧间隔理论上是40ms，但是实践发现并不是，有的是80，有的是120不等。
所以seek后会出现视频会一直等待音频播放，就是视频不动，然后音频一直播放的现象
v1080的海绵宝宝的视频在vpts中11160的附近，有好几帧间隔在200ms，120ms，等所以播放了一帧又要等好久

后又检查发现，没有seek，就是正常播放的时候11160的帧的时间又是正常的。只是时间大小顺序不同。应该dts和pts的原因。



